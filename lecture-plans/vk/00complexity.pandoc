% Сложност и изчислимост
% и практическото им приложение в протоколите/форматите/езиците
% Vasil Kolev \<vasil@ludost.net\>

# Сложност

## Механизми за обработка
### Описание

* Краен автомат
* Стеков автомат
* Turing-complete

<!--
За обработка на различни типове протоколи/формати имаме в общи линии три абстракции, които можем дам ползваме - краен автомат, стеков автомат и машина на Тюринг.
-->
### Краен автомат

* (сравнително) лесен за конструкция
* възможен за валидация
* ограничен като възможности

<!--
Крайните автомати са най-простата възможна конструкция, която в някакви размери е възможна за валидация, но е ограничена като възможности. Един пример за крайни автомати са нормалните регулярни изрази (не тези в perl, където има допълнителни добавки).

В общи линии протокол, който използва специфичен разделител между различните си полета може да се обработва с краен автомат. 
-->

### Примерен протокол за краен автомат - CSV

* Може да се обработва с регулярни изрази
* Coma-separated values
    * Полетата се разделят със запетайка
    * Полетата са оградени с двойни кавички
    * Ако в средата на поле има двойна кавичка, пред нея се поставя \\
* Пример

~~~~~~
1,"pesho","gosho, tosho",tmp\"m,"tmp\"tmp",,
~~~~~~

<!--
Един примерен протокол е CSV - coma-separated values. Представлява стойности, оградени в кавички и разделени с запетаи, като има правила как се решава проблема ако в дадено поле има запетая или кавичка. Това може да се обработва и валидира с прост краен автомат или с регулярни изрази.
-->


### Стеков автомат

* по-сложен за конструкция
* с повече възможности
* вложени структури (json, XML)
* type-length-value схеми
    * повечето мрежови протоколи го използват

<!--
Стековите автомати са по-сложни и се използват за протоколи, в които може да има влагане на различни елементи. Много познати протоколи (като XML, JSON) се обработват със стекови автомати, както и схемите от типа type-length-value, които са повечето мрежови протоколи.

Тези автомати са по-сложни за писане и съответно по-сложни за валидация, което води до доста грешки в тях. Може да се каже, че този тип автомати, заедно с memory management проблемите в C/C++ са един от най-големите източници на пробиви в сигурността на софтуера.

Пример за тривиална атака върху такъв недомислен автомат е да направите пакет, в който имате тип, дължина, която е по-голяма от самия пакет и някакви данни, което при неправилно направен автомат/parser ще доведе до четене на данни извън пакета и странно поведение на програмата.
-->

### Примерен протокол (и проблем) за стеков автомат

* PING протокол с дължина и данни
* Пращате му пакет, той ви връща същия
* Какво ще получите, ако му дадете различна дължина?
    * същия невалиден пакет
    * crash на програмата
    * някакви данни след буфера
    * да не отговори или да отговори с грешка
* http://heartbleed.com/


<!--
Пример за такъв протокол може да се даде с нещо от тип ping - пращате пакет, в който има дължина и данни, и отсрещната страна ви връща същото.

Какво ще се получи обаче, ако дадете по-голяма дължина, отколкото данни? Има различни варианти:
Може да получите същия невалиден пакет (каквото повикало, такова се обадило);
Може да гръмне програмата отсреща, понеже се опитва да чете извън тези данни;
Да върне някакви данни след буфера, в който е получения пакет и да изтече информация;
Да не отговори или да върне грешка.

Само последният вариант е правилен, всичко останало е лошо, като най-лошото е третото. За подобна уязвимост може да видите heartbleed.
-->

### Packet-in-packet

* Друга атака в/у стекови автомати
* Оригинален пакет: Preamble|length|data|end
* Подъл пакет: Preamble|length|(data|preamble|length|baddata|end)|end
    * ... по wireless мрежа

<!--
Това е една атака, описана от Travis Goodspeed преди няколко години. Идеята е, че ако имаме стандартен мрежов протокол и по някаква причина изтървем началото на пакета, има шанс да прочетем нещо, което не трябва в данните му, т.е. да намерим валиден пакет в данните на някой друг. Това важи най-вече за безжични мрежи, където поради зашумяване на ефира може да се изгуби началото на пакета и да се хванат данните като друг пакет.

Практическото приложение на атаката е, че ако накарате някой на другия край на света да свали от вас голям файл и в него има такива под-пакети, има шанс някой около него да види тези под-пакети като истински, например beacon-и за определена wireless мрежа (да подлъжете някой, че има друга мрежа там), или да се възползвате от някоя уязвимост в wireless драйвера (каквито се намират).

-->

### Turing complete автомат

* това, което ни е познато като "компютърна програма"
* практически невъзможен за валидация
    * супер-експоненциално, 2^p^, p=2^x^
* всички програмни езици (които трябва да са такива)
* разни неща, които не трябва
    * HTML5+CSS3

* Примери:
http://beza1e1.tuxen.de/articles/accidentally_turing_complete.html

<!--
Машините на Тюринг, въпреки че са нещо наистина мощно и удобно и са основата на информатиката, която познавам, представляват и един сериозен проблем от гледна точка на сигурността.

Според някои основни теореми, много е трудно да се докаже някакво специфично поведение на машина на Тюринг, използвайки друга машина на Тюринг, т.е. проверката на верността на всяка такава машина трябва да се прави на ръка. Съществуващите методи за това (ако изобщо се ползват) са супер-експоненциални.

Няма да давам пример, понеже ще е сложен (и ще прилича на код от реалния свят). Може да помислите по въпроса какво има в една web страница и колко сложно е реално.

-->

### А какво ползваме ние?

* Реално погледнато - машини на Тюринг
* Всеки автомат, който има странични ефекти (заделяне на памет например) е такъв
* Още по-сложно да се валидира един такъв parser


<!--
Въпреки, че имаме по-прости конструкции, това, което най-често се среща в кода е машина на Тюринг, най-вече защото за програмистите е по-лесно. Дори където имаме краен или стеков автомат, много често в кода имаме т.нар. "странични ефекти", които водят до превръщането на автомата в машина на Тюринг и до всякакви неприятни последици.

Накратко, мъка.
-->
## Последици за нас

### Последици 1/2

* Невъзможност за сигурна обработка на някои
    * Turing-complete нещата
* Огромно количество security проблеми
* Тотално неразбиране на идеята за валидация
* Shotgun-type валидациа

<!--
Това води до много несигурни и опасни parser-и. Някои от тях няма начин на практика да се валидират, някои (които обработват Turing-complete неща) реално нямат как да валидират това, което проверяват, и това ни води до настина много проблеми.

Това идва от неразбирането на идеята за валидация, защото хората предпочитат да мислят, че входните им данни винаги са валидни, и просто добавят по някоя друга проверка за случаи, които им хрумват. Това води до т.нар. shotgun-type валидация, т.е. какво би се случило, ако си разпечатате кода на листи, лепнете го на стената и стрелята от далече по него с пушка със съчми, и където падне някоя съчма - слагате проверка...
-->

### Последици 2/2 (weird machines)

* Много неща се оказват по странен начин Turing-complete
* Вероятно може да напишете нещо с HTML5+CSS3, което смята bitcoin
    * ... или да ползвате принтер с postscript

### Решения

* По-прости протоколи
* Автоматично генерирани parser-и/валидатори
* http://en.wikipedia.org/wiki/Category:Parser_generators
* http://en.wikipedia.org/wiki/Comparison_of_parser_generators

<!--
Проблемът почти има решение.

Първо, може да се мислят/правят протоколи с по-ниска сложност, доколкото е възможно, т.е. които и лесно могат да се parse-нат, с по-малко неясноти в тях и като цяло колкото се може по-малки, понеже колкото по-малко код се налага да пишете, толкова по-малко грешки могат да се допуснат.

Друго нещо, което може (и трябва) да се прави е да се ползват автоматични инструменти за генерирането на parser-и и валидатори. Има едно количество такива, които от зададена формално граматика в някакъв вид могат да генерират код, който да я parse-ва и валидира. Доста от тях са формално доказани (или наистина много добре тествани) и могат да се ползва спокойно. 

Примерни инструменти може да потърсите, wikipedia има цяла категория на parser generators.
-->

## Недомислена изчислителна сложност

### Хешове

* Каква е сложността за добавяне на елемент в хеш?
     * o(n) = log~b~(n) (b - брой bucket-и)
     * обаче, O(n) = n

* Практическо следствие - n.runs-SA-2011.004
http://archives.neohapsis.com/archives/bugtraq/2011-12/0181.html

<!--
И една отделна атака в/у изчислителната сложност - разликата между средната сложност за изпълняване на операция и максималната.

Предполагам, всички знаете какво е хеш, та съвсем накратко - структура, която представлява един твърд масив с пряка адресация от т.нар. bucket-и, към който е закачен свързан списък (или по-рядко - дърво) с елементи. За всеки елемент, който се слага вътре се смята хеш функция, която връща в кой bucket да се добави елемента, а търсенето е в общи линии претърсване на списъка в нужния bucket.

По принцип операциите с хеш се водят с логаритмична сложност - намиране, добавяне, махане на елемент. На практика обаче в най-лошия случай сложността е линейна, понеже ако hash функцията ни е калпава, може всички елементи да ни паднат в същия bucket, което да ни сведе хеша до един свързан списък.
-->

### Атаката в/у хешовете

* Много елементи, които се хешират до същото нещо
* http://a.com/a.php?aa=&bb=&cc=....
* hash(aa)=hash(bb)=hash(cc)=...
* Това води до ~= n^2^ действия
* При n>10000 това отнема бая време


<!--
Преди няколко години двама немци откриха, че ако наслагаме много елементи в същия bucket, то изведнъж всичките операции ще ни станат бавни. Оказа се, че почти всички езици, които имат hash таблици в тях (perl, php, java, ruby и т.н.) ползват подобни функции, за които не е трудно да се пресметнат достатъчно съвпадащи стойности.

Също така, понеже това е проблем в самия език, нямаше начин това да се оправи от самите програмисти - примерът е как някой подава параметри на web приложение, и самия runtime ги слага в hash, още преди да се изпълни кода на програмиста. В този случай просто сървъра ще цикли известно време, докато или не се прекъсне връзката, или не приключи със самата операция.

Това е може би един от най-добрите примери как трябва да се внимава какви примитиви използваме и какъв проблем могат да ни създадат.
-->
